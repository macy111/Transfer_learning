{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  迁移学习 (tensorflow)\n",
    "\n",
    "- 从哪里迁移？\n",
    "    - 源网络：基于imageNet数据集训练好的AlexNet\n",
    "    - 本实验使用的源网络的[参考链接](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/)\n",
    "    - 权重文件为 bvlc_alexnet.npy （可从上面[链接](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/)处点击下载）\n",
    "- 迁移到哪里？\n",
    "    - 目标数据集：cifar10\n",
    "- 怎么迁移？\n",
    "    - 去除AlexNet最后一层（原来的1000个类别的输出层）\n",
    "    - 加上神经元个数为10的输出层（cifar10有10个类别）\n",
    "    - 加载前面所有层的源网络上训练好的权重\n",
    "    - 随机初始化最后一层权重\n",
    "    - 训练最后一层\n",
    " \n",
    "构建网络的部分代码参考了[该链接](https://github.com/kratzert/finetune_alexnet_with_tensorflow/tree/5d751d62eb4d7149f4e3fd465febf8f07d4cea9d)，在此基础上做了些调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.datasets.cifar10 as cifar10\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些提升代码可重用性的辅助方法\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,padding='SAME', groups=1):\n",
    "\n",
    "    # Get number of input channels\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k,\n",
    "                                   strides = [1, stride_y, stride_x, 1],\n",
    "                                   padding = padding)\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape = [filter_height, filter_width,\n",
    "                                  input_channels/groups, num_filters])\n",
    "        biases = tf.get_variable('biases', shape = [num_filters])\n",
    "\n",
    "\n",
    "        if groups == 1:\n",
    "            conv = convolve(x, weights)\n",
    "\n",
    "        # In the cases of multiple groups, split inputs & weights and\n",
    "        else:\n",
    "            # Split input and weights and convolve them separately\n",
    "            input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\n",
    "            weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\n",
    "            output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\n",
    "\n",
    "            # Concat the convolved output together again\n",
    "            conv = tf.concat(axis = 3, values = output_groups)\n",
    "\n",
    "        # Add biases\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "        # Apply relu function\n",
    "        relu = tf.nn.relu(bias, name = scope.name)\n",
    "\n",
    "        return relu\n",
    "\n",
    "# 全连接层\n",
    "def fc(x, num_in, num_out, name, relu = True):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "        if relu == True:\n",
    "            # Apply ReLu non linearity\n",
    "            relu = tf.nn.relu(act)      \n",
    "            return relu\n",
    "        else:\n",
    "            return act\n",
    "    \n",
    "# 池化层\n",
    "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
    "                        strides = [1, stride_y, stride_x, 1],\n",
    "                        padding = padding, name = name)\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "    return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\n",
    "                                            beta = beta, bias = bias, name = name)\n",
    "  \n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建网络\n",
    "注意：网络结构中迁移自别人网络的那部分网络，需要和别人训练时使用的网络结构一致，这样才可以方便我们载入别人预训练好的权重。假设别人提供了网络结构代码与权重文件，那我们就创建一样的网络结构并导入权重（例子中的情况）。若别人提供了网络结构meta后缀的文件和ckpt权重文件，那我们就需要以对应的从meta文件导入网络的方式创建网络。总而言之，要一一对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(object):\n",
    "    \n",
    "    def __init__(self, x, keep_prob, num_classes, skip_layer, weights_path = 'DEFAULT'):\n",
    "    \n",
    "        # 初始化相关参数\n",
    "        self.X = x\n",
    "        self.NUM_CLASSES = num_classes\n",
    "        self.KEEP_PROB = keep_prob\n",
    "        self.SKIP_LAYER = skip_layer #不加载预训练参数的层\n",
    "\n",
    "        if weights_path == 'DEFAULT':      \n",
    "            self.WEIGHTS_PATH = 'bvlc_alexnet.npy' #默认权重文件路径\n",
    "        else:\n",
    "            self.WEIGHTS_PATH = weights_path #自定义权重文件路径\n",
    "\n",
    "        # 创建网络\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "\n",
    "        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\n",
    "        norm1 = lrn(conv1, 2, 1e-05, 0.75, name = 'norm1')\n",
    "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\n",
    "\n",
    "        # 2nd Layer: Conv (w ReLu) -> Lrn -> Poolwith 2 groups\n",
    "        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\n",
    "        norm2 = lrn(conv2, 2, 1e-05, 0.75, name = 'norm2')\n",
    "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\n",
    "\n",
    "        # 3rd Layer: Conv (w ReLu)\n",
    "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name = 'conv3')\n",
    "\n",
    "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\n",
    "\n",
    "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\n",
    "\n",
    "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "        flattened = tf.reshape(pool5, [-1, 6*6*256])\n",
    "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
    "        dropout6 = dropout(fc6, self.KEEP_PROB)\n",
    "\n",
    "        # 7th Layer: FC (w ReLu) -> Dropout\n",
    "        fc7 = fc(dropout6, 4096, 4096, name = 'fc7')\n",
    "        dropout7 = dropout(fc7, self.KEEP_PROB)\n",
    "\n",
    "        # 8th Layer: FC and return unscaled activations\n",
    "        # (for tf.nn.softmax_cross_entropy_with_logits)\n",
    "        # 和原始网络不一样的一层， 输出 self.NUM_CLASSES 的大小为10 而不是原来的1000\n",
    "        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\n",
    "\n",
    "    \n",
    "    def load_initial_weights(self, session):\n",
    "        \"\"\"\n",
    "        对需要加载预先训练好的权重的层，从bvlc_alexnet.npy加载预先训练好的权重到网络中来。\n",
    "        \"\"\"\n",
    "\n",
    "        # 加载权重文件\n",
    "        weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\n",
    "        \n",
    "        # 循环所有的层\n",
    "        for op_name in weights_dict:\n",
    "            # 检查当前层是否是我们想要加载预训练好的权重并冻结权重不进行训练的层\n",
    "            if op_name not in self.SKIP_LAYER:\n",
    "                with tf.variable_scope(op_name, reuse = True):\n",
    "                    # 加载参数到对应的层\n",
    "                    for data in weights_dict[op_name]:\n",
    "                        # Biases\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable = False)\n",
    "                            session.run(var.assign(data))\n",
    "                        # Weights\n",
    "                        else:\n",
    "                            var = tf.get_variable('weights', trainable = False)\n",
    "                            session.run(var.assign(data))\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载 cifar10 数据集\n",
    "首次加载时会自动从网络自动下载该数据集，非首次加载时会自动从本地读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通道转换 RGB -> BGR\n",
    "cifar10.load_data()获得的数据集是RGB通道的，但是我们迁移的源网络则是基于BGR通道的图片进行训练的，因此，首先我们需要进行通道转换的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:, :, :, 0], x_train[:, :, :, 2] = x_train[:, :, :, 2], x_train[:, :, :, 0].copy()\n",
    "x_test[:, :, :, 0], x_test[:, :, :, 2] = x_test[:, :, :, 2], x_test[:, :, :, 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练数据集一共有50000张图片\n",
    "- 测试数据集一共有10000张图片\n",
    "- 图片大小 32 * 32 * 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图片生成器\n",
    "包括对图片进行翻转，resize, 去均值化，one_hot编码等一系列预处理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGenerator:\n",
    "    def __init__(self, x, y, horizontal_flip=False, shuffle=False, \n",
    "                 mean = np.array([104., 117., 124.]), scale_size=(227, 227),\n",
    "                 nb_classes = 10):\n",
    "                \n",
    "        # 初始化参数\n",
    "        self.horizontal_flip = horizontal_flip #是否水平翻转\n",
    "        self.n_classes = nb_classes #输出类别数\n",
    "        self.shuffle = shuffle #是否打乱数据\n",
    "        self.mean = mean #图片均值，以便进行去均值化处理\n",
    "        self.scale_size = scale_size #图片resize后的尺寸\n",
    "        self.pointer = 0\n",
    "        \n",
    "        self.images = x \n",
    "        self.labels = y\n",
    "        self.data_size = len(self.labels)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"\n",
    "        打乱数据\n",
    "        \"\"\"\n",
    "        images = self.images.copy()\n",
    "        labels = self.labels.copy()\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        idx = np.random.permutation(len(labels))\n",
    "        for i in idx:\n",
    "            self.images.append(images[i])\n",
    "            self.labels.append(labels[i])\n",
    "                \n",
    "    def reset_pointer(self):\n",
    "        \"\"\"\n",
    "        重置指针指向初始位置\n",
    "        \"\"\"\n",
    "        self.pointer = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        \n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        生成一批量的数据\n",
    "        \"\"\"\n",
    "        # 获取一批图片\n",
    "        batch_images = self.images[self.pointer:self.pointer + batch_size]\n",
    "        batch_labels = self.labels[self.pointer:self.pointer + batch_size]\n",
    "        \n",
    "        # 更新指针\n",
    "        self.pointer += batch_size\n",
    "        \n",
    "        images = np.ndarray([batch_size, self.scale_size[0], self.scale_size[1], 3])\n",
    "        \n",
    "        one_hot_labels = np.zeros((batch_size, self.n_classes))\n",
    "        for i in range(len(batch_images)):\n",
    "            img = batch_images[i]\n",
    "            \n",
    "            # 随机水平翻转图片\n",
    "            if self.horizontal_flip and np.random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "            \n",
    "            # resize图片大小\n",
    "            img = cv2.resize(img, (self.scale_size[0], self.scale_size[1]))\n",
    "            img = img.astype(np.float32)\n",
    "            \n",
    "            # 去均值化处理\n",
    "            img -= self.mean\n",
    "                                                                 \n",
    "            images[i] = img\n",
    "            \n",
    "            # 对结果进行one-hot编码 \n",
    "            one_hot_labels[i][batch_labels[i]] = 1\n",
    "\n",
    "\n",
    "        # return 一批预处理完后的图片和对应标签\n",
    "        return images, one_hot_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:46:38.041832 Start training...\n",
      "2018-12-03 14:46:38.041919 Epoch number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [01:50<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:48:29.331706 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:48:51.337752 Validation Accuracy = 0.6944\n",
      "2018-12-03 14:48:51.356271 Epoch number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:54<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:50:46.444825 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:51:09.280474 Validation Accuracy = 0.7313\n",
      "2018-12-03 14:51:09.301053 Epoch number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:54<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:53:04.115745 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:53:26.224274 Validation Accuracy = 0.6962\n",
      "2018-12-03 14:53:26.244045 Epoch number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:50<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:55:17.016173 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:55:39.044961 Validation Accuracy = 0.7341\n",
      "2018-12-03 14:55:39.064166 Epoch number: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:49<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:57:29.594900 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:57:51.650191 Validation Accuracy = 0.7278\n",
      "2018-12-03 14:57:51.670206 Epoch number: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:51<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 14:59:43.300880 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:00:05.448166 Validation Accuracy = 0.7042\n",
      "2018-12-03 15:00:05.467255 Epoch number: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:50<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:01:56.377345 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:02:18.498373 Validation Accuracy = 0.7549\n",
      "2018-12-03 15:02:18.518558 Epoch number: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:50<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:04:09.585243 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:04:31.692118 Validation Accuracy = 0.7241\n",
      "2018-12-03 15:04:31.711660 Epoch number: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:51<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:06:23.521184 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:06:45.675578 Validation Accuracy = 0.7322\n",
      "2018-12-03 15:06:45.694950 Epoch number: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 194/194 [01:51<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:08:37.604543 Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [00:22<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 15:08:59.767560 Validation Accuracy = 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 256 # 若电脑因为显存大小不够或内存大小不够而报错，请减小batch_size\n",
    "\n",
    "dropout_rate = 0.5\n",
    "num_classes = 10 # 最后一层的输出类别数\n",
    "\n",
    "# train_layers 指定哪些网络层是需要训练的\n",
    "train_layers = ['fc8'] # 此处仅训练最后新添加的一层的参数\n",
    "#train_layers = ['fc8', 'fc7', 'fc6', 'fc7', 'conv5', 'conv4', 'conv3', 'conv2', 'conv1'] # 此处被注释掉的地方是为了对比实验存在的\n",
    "\n",
    "\n",
    "# 网络的输入与输出\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 模型初始化\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# score指向模型的输出\n",
    "score = model.fc8\n",
    "\n",
    "# 获取所有待训练的 variables\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# 计算损失函数\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = y))  \n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # 获取梯度\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # 对可训练的层使用GradientDescentOptimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "\n",
    "# 计算accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "# 初始化数据生成器\n",
    "train_generator = ImageDataGenerator(x_train,y_train, \n",
    "                                     horizontal_flip = True, shuffle = True)\n",
    "val_generator = ImageDataGenerator(x_test,y_test, shuffle = False) \n",
    "\n",
    "# 计算每一个epoch的step数\n",
    "train_batches_per_epoch = np.floor(train_generator.data_size / batch_size).astype(np.int16)\n",
    "val_batches_per_epoch = np.floor(val_generator.data_size / batch_size).astype(np.int16)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 加载模型参数\n",
    "    model.load_initial_weights(sess)\n",
    "\n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "\n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "        sys.stdout.flush()\n",
    "        step = 1\n",
    "        pbar = tqdm(total=train_batches_per_epoch-1)\n",
    "    \n",
    "        while step < train_batches_per_epoch:\n",
    "            pbar.update(1)\n",
    "            # Get a batch of images and labels\n",
    "            batch_xs, batch_ys = train_generator.next_batch(batch_size)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: batch_xs, \n",
    "                                          y: batch_ys, \n",
    "                                          keep_prob: dropout_rate})\n",
    "            step += 1\n",
    "        pbar.close()\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sys.stdout.flush()\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        for _ in tqdm(range(val_batches_per_epoch)):\n",
    "            batch_tx, batch_ty = val_generator.next_batch(batch_size)\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_tx, \n",
    "                                                y: batch_ty, \n",
    "                                                keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), test_acc))\n",
    "\n",
    "        # Reset the file pointer of the image data generator\n",
    "        val_generator.reset_pointer()\n",
    "        train_generator.reset_pointer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowGpu]",
   "language": "python",
   "name": "conda-env-tensorflowGpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
